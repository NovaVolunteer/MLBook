{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression Overview\n",
        "\n",
        "**Logistic Regression** is a supervised learning algorithm used for binary classification problems, where the target variable can take on only two possible outcomes (e.g., \"yes\" or \"no\", \"pass\" or \"fail\", \"spam\" or \"not spam\"). Despite its name containing \"regression,\" logistic regression is actually a classification algorithm.\n",
        "\n",
        "## Real-World Applications\n",
        "\n",
        "Logistic regression is widely used across many domains:\n",
        "\n",
        "  + **Medicine**: Predicting whether a patient has a disease based on symptoms and test results\n",
        "  + **Finance**: Determining credit risk (will a customer default on a loan?)\n",
        "  + **Marketing**: Predicting customer churn (will a customer leave the service?)\n",
        "  + **Email Filtering**: Classifying emails as spam or not spam\n",
        "  + **Admissions**: Predicting whether a student will be admitted to a university\n",
        "\n",
        "In these scenarios, we're not predicting a continuous value like in linear regression, but rather the probability that an instance belongs to a particular class.\n",
        "\n",
        "## From Linear Regression to Logistic Regression\n",
        "\n",
        "To understand logistic regression, let's first recall linear regression:\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
        "$$\n",
        "\n",
        "Linear regression predicts continuous values and can output any real number. However, for classification, we need to predict probabilities that are bounded between 0 and 1.\n",
        "\n",
        "## The Sigmoid (Logistic) Function\n",
        "\n",
        "The key to logistic regression is the **sigmoid function** (also called the logistic function), which transforms any real-valued number into a value between 0 and 1:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "  + $z$ is the linear combination of input features: $z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n$\n",
        "  + $e$ is Euler's number (approximately 2.71828)\n",
        "  + $\\sigma(z)$ represents the probability that $y = 1$ given the input features\n",
        "\n",
        "The complete logistic regression model is:\n",
        "\n",
        "$$\n",
        "P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n",
        "$$\n",
        "\n",
        "### Properties of the Sigmoid Function\n",
        "\n",
        "The sigmoid function has several important properties:\n",
        "\n",
        "  + When $z \\to \\infty$, $\\sigma(z) \\to 1$\n",
        "  + When $z \\to -\\infty$, $\\sigma(z) \\to 0$\n",
        "  + When $z = 0$, $\\sigma(z) = 0.5$\n",
        "  + The function is smooth and differentiable everywhere (important for gradient-based optimization)\n",
        "  + It has an S-shaped curve\n",
        "\n",
        "Let's visualize the sigmoid function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Create z values\n",
        "z = np.linspace(-10, 10, 200)\n",
        "y = sigmoid(z)\n",
        "\n",
        "# Create the plot\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=z, y=y, mode='lines', name='Sigmoid Function',\n",
        "                         line=dict(color='blue', width=2)))\n",
        "\n",
        "# Add a horizontal line at y=0.5 for the decision boundary\n",
        "fig.add_hline(y=0.5, line_dash=\"dash\", line_color=\"red\", \n",
        "              annotation_text=\"Decision Boundary (p=0.5)\")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"The Sigmoid (Logistic) Function\",\n",
        "    xaxis_title=\"z (Linear Combination of Features)\",\n",
        "    yaxis_title=\"σ(z) - Probability\",\n",
        "    height=400\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log-Odds and the Logit Function\n",
        "\n",
        "To understand logistic regression from a probability perspective, we introduce the concept of **odds** and **log-odds**.\n",
        "\n",
        "### Odds\n",
        "\n",
        "If the probability of an event is $p$, then the **odds** of that event are:\n",
        "\n",
        "$$\n",
        "\\text{Odds} = \\frac{p}{1-p}\n",
        "$$\n",
        "\n",
        "For example, if the probability of rain is 0.75, the odds are $\\frac{0.75}{0.25} = 3$, meaning rain is 3 times more likely than no rain.\n",
        "\n",
        "### Log-Odds (Logit)\n",
        "\n",
        "The **log-odds** (or **logit**) is the natural logarithm of the odds:\n",
        "\n",
        "$$\n",
        "\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\n",
        "$$\n",
        "\n",
        "In logistic regression, we model the log-odds as a linear function of the input features:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{P(y=1|X)}{1-P(y=1|X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
        "$$\n",
        "\n",
        "This is why logistic regression is also called a **Generalized Linear Model (GLM)** - it models a transformation of the probability (the log-odds) as a linear combination of features.\n",
        "\n",
        "By taking the exponential of both sides and rearranging, we arrive back at the sigmoid function we saw earlier.\n",
        "\n",
        "## Mathematical Connections\n",
        "\n",
        "Logistic regression connects to several mathematical concepts from your previous coursework:\n",
        "\n",
        "### Probability Theory\n",
        "\n",
        "  + The output is a probability: $P(y=1|X) \\in [0,1]$\n",
        "  + We use the Bernoulli distribution (binary outcomes)\n",
        "  + Maximum Likelihood Estimation (MLE) is used to find optimal parameters\n",
        "\n",
        "### Calculus\n",
        "\n",
        "  + The derivative of the sigmoid function has a convenient form:\n",
        "\n",
        "$$\n",
        "\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1 - \\sigma(z))\n",
        "$$\n",
        "\n",
        "  + This makes gradient descent optimization efficient\n",
        "  + The loss function involves logarithms and requires calculus to minimize\n",
        "\n",
        "### Linear Algebra\n",
        "\n",
        "  + The linear combination of features can be written as a dot product:\n",
        "\n",
        "$$\n",
        "z = \\boldsymbol{\\beta}^T \\mathbf{X} = \\beta_0 + \\sum_{i=1}^{n} \\beta_i X_i\n",
        "$$\n",
        "\n",
        "  + Matrix operations are used for efficient computation with multiple samples\n",
        "\n",
        "## The Loss Function: Binary Cross-Entropy\n",
        "\n",
        "To train a logistic regression model, we need to define a loss function that measures how well our predictions match the actual labels. The standard loss function for logistic regression is **binary cross-entropy** (also called log loss):\n",
        "\n",
        "For a single observation:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{p}) = -[y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "  + $y$ is the actual label (0 or 1)\n",
        "  + $\\hat{p}$ is the predicted probability that $y=1$\n",
        "\n",
        "For the entire dataset, we take the average:\n",
        "\n",
        "$$\n",
        "J(\\boldsymbol{\\beta}) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{p}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{p}^{(i)})]\n",
        "$$\n",
        "\n",
        "Where $m$ is the number of training examples.\n",
        "\n",
        "This loss function:\n",
        "  + Heavily penalizes confident wrong predictions\n",
        "  + Is derived from the principle of Maximum Likelihood Estimation\n",
        "  + Is convex, which guarantees a global minimum\n",
        "\n",
        "## Model Training: Gradient Descent\n",
        "\n",
        "Like linear regression, logistic regression uses **gradient descent** to find the optimal parameters $\\boldsymbol{\\beta}$ that minimize the loss function.\n",
        "\n",
        "The update rule for each parameter is:\n",
        "\n",
        "$$\n",
        "\\beta_j := \\beta_j - \\alpha \\frac{\\partial J(\\boldsymbol{\\beta})}{\\partial \\beta_j}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "  + $\\alpha$ is the learning rate\n",
        "  + The gradient (partial derivative) tells us the direction to adjust the parameter\n",
        "\n",
        "For logistic regression, the gradient has the form:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\boldsymbol{\\beta})}{\\partial \\beta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{p}^{(i)} - y^{(i)}) x_j^{(i)}\n",
        "$$\n",
        "\n",
        "Notice this looks similar to linear regression, but $\\hat{p}$ comes from the sigmoid function!\n",
        "\n",
        "## Making Predictions\n",
        "\n",
        "Once trained, logistic regression makes predictions in two steps:\n",
        "\n",
        "1. **Calculate the probability**: \n",
        "$$\n",
        "\\hat{p} = P(y=1|X) = \\sigma(\\boldsymbol{\\beta}^T \\mathbf{X})\n",
        "$$\n",
        "\n",
        "2. **Apply a decision threshold** (typically 0.5):\n",
        "$$\n",
        "\\hat{y} = \\begin{cases} \n",
        "1 & \\text{if } \\hat{p} \\geq 0.5 \\\\\n",
        "0 & \\text{if } \\hat{p} < 0.5\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The threshold can be adjusted based on the problem requirements:\n",
        "  + Higher threshold: More conservative predictions of the positive class (fewer false positives)\n",
        "  + Lower threshold: More aggressive predictions of the positive class (fewer false negatives)\n",
        "\n",
        "## Logistic Regression in Python\n",
        "\n",
        "Let's implement logistic regression using scikit-learn with a practical example. We'll create a dataset to predict whether a student will pass an exam based on hours studied and previous test scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a sample dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "\n",
        "# Features: hours studied and previous test score\n",
        "hours_studied = np.random.uniform(0, 10, n_samples)\n",
        "previous_score = np.random.uniform(40, 100, n_samples)\n",
        "\n",
        "# Target: pass (1) or fail (0)\n",
        "# Higher hours and previous scores increase probability of passing\n",
        "z = -8 + 0.7 * hours_studied + 0.1 * previous_score + np.random.normal(0, 1, n_samples)\n",
        "probability = 1 / (1 + np.exp(-z))\n",
        "passed = (probability > 0.5).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'HoursStudied': hours_studied,\n",
        "    'PreviousScore': previous_score,\n",
        "    'Passed': passed\n",
        "})\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(df.head(10))\n",
        "print(f\"\\nPass Rate: {df['Passed'].mean():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualization\n",
        "\n",
        "Let's visualize the relationship between our features and the target variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df, x='HoursStudied', y='PreviousScore', \n",
        "                 color='Passed', \n",
        "                 color_discrete_map={0: 'red', 1: 'green'},\n",
        "                 labels={'Passed': 'Exam Result'},\n",
        "                 title='Student Exam Results by Study Hours and Previous Score',\n",
        "                 height=450)\n",
        "\n",
        "fig.update_traces(marker=dict(size=8, line=dict(width=1, color='DarkSlateGrey')))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting\n",
        "\n",
        "We split our data into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df[['HoursStudied', 'PreviousScore']]\n",
        "y = df['Passed']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Scaling\n",
        "\n",
        ":::{.callout-note title=\"Feature Scaling\"}\n",
        "Feature scaling is important for logistic regression because:\n",
        "  + It helps gradient descent converge faster\n",
        "  + It ensures all features contribute equally to the model\n",
        "  + It's especially important when features are on different scales\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling applied\")\n",
        "print(f\"Original HoursStudied range: [{X_train['HoursStudied'].min():.2f}, {X_train['HoursStudied'].max():.2f}]\")\n",
        "print(f\"Scaled HoursStudied range: [{X_train_scaled[:, 0].min():.2f}, {X_train_scaled[:, 0].max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training\n",
        "\n",
        "Now we train the logistic regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train the model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Display the learned parameters\n",
        "print(\"Model Coefficients:\")\n",
        "print(f\"  Intercept (β₀): {log_reg.intercept_[0]:.4f}\")\n",
        "print(f\"  HoursStudied (β₁): {log_reg.coef_[0][0]:.4f}\")\n",
        "print(f\"  PreviousScore (β₂): {log_reg.coef_[0][1]:.4f}\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"  A one standard deviation increase in HoursStudied increases\")\n",
        "print(f\"  the log-odds of passing by {log_reg.coef_[0][0]:.4f}\")\n",
        "print(f\"  A one standard deviation increase in PreviousScore increases\")\n",
        "print(f\"  the log-odds of passing by {log_reg.coef_[0][1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making Predictions\n",
        "\n",
        "Logistic regression provides both predicted probabilities and class labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predicted probabilities\n",
        "y_pred_proba = log_reg.predict_proba(X_test_scaled)\n",
        "\n",
        "# Get predicted class labels\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Create a results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'HoursStudied': X_test['HoursStudied'].values,\n",
        "    'PreviousScore': X_test['PreviousScore'].values,\n",
        "    'Actual': y_test.values,\n",
        "    'Prob_Fail': y_pred_proba[:, 0],\n",
        "    'Prob_Pass': y_pred_proba[:, 1],\n",
        "    'Predicted': y_pred\n",
        "})\n",
        "\n",
        "print(\"Sample Predictions:\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics for Classification\n",
        "\n",
        "Unlike regression, we use different metrics to evaluate classification models:\n",
        "\n",
        "### Confusion Matrix\n",
        "\n",
        "The **confusion matrix** shows the counts of correct and incorrect predictions:\n",
        "\n",
        "| | Predicted Negative (0) | Predicted Positive (1) |\n",
        "|---|---|---|\n",
        "| **Actual Negative (0)** | True Negative (TN) | False Positive (FP) |\n",
        "| **Actual Positive (1)** | False Negative (FN) | True Positive (TP) |\n",
        "\n",
        "### Key Metrics\n",
        "\n",
        "From the confusion matrix, we derive several important metrics:\n",
        "\n",
        "**Accuracy**: Overall proportion of correct predictions\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "**Precision**: Of all positive predictions, what proportion was actually positive?\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "**Recall** (Sensitivity): Of all actual positives, what proportion was correctly identified?\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "**F1-Score**: Harmonic mean of precision and recall\n",
        "$$\n",
        "F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "Let's calculate these metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nTrue Negatives: {cm[0,0]}\")\n",
        "print(f\"False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}\")\n",
        "print(f\"True Positives: {cm[1,1]}\")\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(y_test, y_pred, target_names=['Failed', 'Passed']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Create confusion matrix heatmap\n",
        "z = cm\n",
        "x = ['Predicted: Failed (0)', 'Predicted: Passed (1)']\n",
        "y = ['Actual: Failed (0)', 'Actual: Passed (1)']\n",
        "\n",
        "# Create annotations for each cell\n",
        "z_text = [[str(y) for y in x] for x in z]\n",
        "\n",
        "fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text,\n",
        "                                  colorscale='Blues', showscale=True)\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Confusion Matrix',\n",
        "    xaxis_title='Predicted Label',\n",
        "    yaxis_title='Actual Label',\n",
        "    height=400\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROC Curve and AUC\n",
        "\n",
        "The **Receiver Operating Characteristic (ROC)** curve is a graphical plot that shows the trade-off between the true positive rate (recall) and false positive rate at various threshold settings.\n",
        "\n",
        "The **Area Under the Curve (AUC)** provides a single number summary of model performance:\n",
        "  + AUC = 1.0: Perfect classifier\n",
        "  + AUC = 0.5: Random classifier (no better than chance)\n",
        "  + AUC < 0.5: Worse than random (predictions are inverted)\n",
        "\n",
        "The ROC curve helps you choose the optimal threshold for your specific application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "\n",
        "# Create ROC curve plot\n",
        "fig = go.Figure()\n",
        "\n",
        "# ROC curve\n",
        "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',\n",
        "                         name=f'ROC Curve (AUC = {auc_score:.3f})',\n",
        "                         line=dict(color='blue', width=2)))\n",
        "\n",
        "# Diagonal line (random classifier)\n",
        "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines',\n",
        "                         name='Random Classifier',\n",
        "                         line=dict(color='red', width=2, dash='dash')))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='ROC Curve',\n",
        "    xaxis_title='False Positive Rate',\n",
        "    yaxis_title='True Positive Rate (Recall)',\n",
        "    height=450,\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(f\"\\nAUC Score: {auc_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Boundary Visualization\n",
        "\n",
        "For a two-feature model, we can visualize the decision boundary - the line that separates the two classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a mesh grid\n",
        "h = 0.02  # step size in the mesh\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predict for each point in the mesh\n",
        "Z = log_reg.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Create contour plot\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add contour for probabilities\n",
        "fig.add_trace(go.Contour(x=xx[0], y=yy[:, 0], z=Z,\n",
        "                         colorscale='RdYlGn',\n",
        "                         contours=dict(start=0, end=1, size=0.1),\n",
        "                         name='Probability',\n",
        "                         showscale=True,\n",
        "                         colorbar=dict(title='P(Pass)')))\n",
        "\n",
        "# Add training points\n",
        "train_pass = y_train == 1\n",
        "train_fail = y_train == 0\n",
        "\n",
        "fig.add_trace(go.Scatter(x=X_train_scaled[train_pass, 0],\n",
        "                         y=X_train_scaled[train_pass, 1],\n",
        "                         mode='markers',\n",
        "                         marker=dict(color='green', size=8, symbol='circle',\n",
        "                                   line=dict(color='darkgreen', width=1)),\n",
        "                         name='Passed (Train)'))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=X_train_scaled[train_fail, 0],\n",
        "                         y=X_train_scaled[train_fail, 1],\n",
        "                         mode='markers',\n",
        "                         marker=dict(color='red', size=8, symbol='x',\n",
        "                                   line=dict(color='darkred', width=1)),\n",
        "                         name='Failed (Train)'))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Decision Boundary and Probability Contours',\n",
        "    xaxis_title='Hours Studied (Scaled)',\n",
        "    yaxis_title='Previous Score (Scaled)',\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiclass Logistic Regression\n",
        "\n",
        "While we've focused on binary classification, logistic regression can be extended to multiclass problems using:\n",
        "\n",
        "  + **One-vs-Rest (OvR)**: Train one classifier per class, treating it as the positive class and all others as negative\n",
        "  + **Multinomial/Softmax**: Generalize the sigmoid function to handle multiple classes simultaneously\n",
        "\n",
        "Scikit-learn's `LogisticRegression` automatically handles multiclass classification using the OvR strategy by default.\n",
        "\n",
        "## Regularization in Logistic Regression\n",
        "\n",
        "Like linear regression, logistic regression can benefit from **regularization** to prevent overfitting:\n",
        "\n",
        "### L2 Regularization (Ridge)\n",
        "\n",
        "Adds a penalty term to the loss function:\n",
        "$$\n",
        "J(\\boldsymbol{\\beta}) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{p}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{p}^{(i)})] + \\lambda \\sum_{j=1}^{n} \\beta_j^2\n",
        "$$\n",
        "\n",
        "### L1 Regularization (Lasso)\n",
        "\n",
        "Uses absolute values instead:\n",
        "$$\n",
        "J(\\boldsymbol{\\beta}) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{p}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{p}^{(i)})] + \\lambda \\sum_{j=1}^{n} |\\beta_j|\n",
        "$$\n",
        "\n",
        "In scikit-learn, the regularization strength is controlled by the parameter `C`, where smaller values specify stronger regularization.\n",
        "\n",
        "## Assumptions and Limitations\n",
        "\n",
        "Logistic regression makes several assumptions:\n",
        "\n",
        "1. **Linear relationship**: The log-odds should be linearly related to the features\n",
        "2. **Independence**: Observations should be independent\n",
        "3. **No multicollinearity**: Features should not be highly correlated\n",
        "4. **Large sample size**: More reliable with larger datasets\n",
        "\n",
        "Limitations:\n",
        "  + Cannot capture complex non-linear relationships (without feature engineering)\n",
        "  + Sensitive to outliers\n",
        "  + Assumes linear decision boundaries\n",
        "\n",
        "## Summary\n",
        "\n",
        "Logistic Regression is a foundational classification algorithm that:\n",
        "\n",
        "  + Uses the sigmoid function to model probabilities\n",
        "  + Learns through gradient descent on the binary cross-entropy loss\n",
        "  + Provides interpretable coefficients showing feature importance\n",
        "  + Works well for linearly separable problems\n",
        "  + Serves as a building block for more complex models\n",
        "\n",
        "Key takeaways:\n",
        "  + The output is a probability, not a continuous value\n",
        "  + We model the log-odds as a linear function of features\n",
        "  + Evaluation uses classification metrics (accuracy, precision, recall, F1, AUC)\n",
        "  + The decision threshold can be adjusted based on application requirements\n",
        "  + Regularization helps prevent overfitting\n",
        "\n",
        "Despite its simplicity, logistic regression remains widely used in practice due to its efficiency, interpretability, and effectiveness on many real-world problems."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
