{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Implementation Examples\n",
    "\n",
    "In this notebook, we will implement Principal Component Analysis in two ways:\n",
    "\n",
    "1. **From Scratch:** Using NumPy to understand the mathematics behind PCA\n",
    "2. **Using Scikit-learn:** Leveraging the efficient PCA implementation from the library\n",
    "\n",
    "We'll use the famous Iris dataset to demonstrate PCA in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Data\n",
    "\n",
    "First, let's load the Iris dataset and examine its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Target: species (0, 1, 2)\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(y, target_names)\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the relationships between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of two features\n",
    "fig = px.scatter(df, x='sepal length (cm)', y='sepal width (cm)', \n",
    "                 color='species', title='Iris Dataset: Sepal Length vs Width',\n",
    "                 height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: PCA Implementation from Scratch\n",
    "\n",
    "Now, let's implement PCA step by step without using any PCA libraries.\n",
    "\n",
    "### Step 1: Standardize the Data\n",
    "\n",
    "We need to standardize the features to have mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual standardization\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_standardized = (X - X_mean) / X_std\n",
    "\n",
    "print(\"Original data mean:\", X_mean)\n",
    "print(\"Original data std:\", X_std)\n",
    "print(\"\\nStandardized data mean:\", np.mean(X_standardized, axis=0))\n",
    "print(\"Standardized data std:\", np.std(X_standardized, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute the Covariance Matrix\n",
    "\n",
    "The covariance matrix captures how features vary together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix manually\n",
    "n_samples = X_standardized.shape[0]\n",
    "covariance_matrix = (X_standardized.T @ X_standardized) / (n_samples - 1)\n",
    "\n",
    "print(\"Covariance Matrix:\")\n",
    "print(covariance_matrix)\n",
    "print(\"\\nShape:\", covariance_matrix.shape)\n",
    "\n",
    "# Visualize the covariance matrix\n",
    "fig = px.imshow(covariance_matrix, \n",
    "                x=feature_names, y=feature_names,\n",
    "                labels=dict(color=\"Covariance\"),\n",
    "                title=\"Covariance Matrix of Iris Features\",\n",
    "                color_continuous_scale='RdBu_r',\n",
    "                aspect='auto', height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute Eigenvalues and Eigenvectors\n",
    "\n",
    "The eigenvectors represent the principal components (directions of maximum variance), and the eigenvalues represent the amount of variance explained by each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors (columns are eigenvectors):\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Verify the eigenvalue equation: Σv = λv\n",
    "print(\"\\nVerification: Σv = λv for first eigenvector:\")\n",
    "v1 = eigenvectors[:, 0]\n",
    "lambda1 = eigenvalues[0]\n",
    "print(\"Σv =\", covariance_matrix @ v1)\n",
    "print(\"λv =\", lambda1 * v1)\n",
    "print(\"Are they equal?\", np.allclose(covariance_matrix @ v1, lambda1 * v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Sort Eigenvalues and Select Principal Components\n",
    "\n",
    "We sort the eigenvalues in descending order and select the corresponding eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort eigenvalues and eigenvectors in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[sorted_indices]\n",
    "eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "\n",
    "print(\"Sorted Eigenvalues:\")\n",
    "print(eigenvalues_sorted)\n",
    "\n",
    "# Calculate variance explained by each component\n",
    "total_variance = np.sum(eigenvalues_sorted)\n",
    "variance_explained = eigenvalues_sorted / total_variance\n",
    "cumulative_variance = np.cumsum(variance_explained)\n",
    "\n",
    "print(\"\\nVariance Explained by Each Component:\")\n",
    "for i, (var, cum_var) in enumerate(zip(variance_explained, cumulative_variance)):\n",
    "    print(f\"PC{i+1}: {var:.4f} ({var*100:.2f}%) - Cumulative: {cum_var:.4f} ({cum_var*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of variance explained\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Variance explained by each component\n",
    "ax1.bar(range(1, len(variance_explained) + 1), variance_explained, alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Variance Explained Ratio')\n",
    "ax1.set_title('Variance Explained by Each Principal Component')\n",
    "ax1.set_xticks(range(1, len(variance_explained) + 1))\n",
    "\n",
    "# Cumulative variance explained\n",
    "ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='steelblue')\n",
    "ax2.axhline(y=0.95, color='r', linestyle='--', label='95% Threshold')\n",
    "ax2.set_xlabel('Number of Principal Components')\n",
    "ax2.set_ylabel('Cumulative Variance Explained')\n",
    "ax2.set_title('Cumulative Variance Explained')\n",
    "ax2.set_xticks(range(1, len(cumulative_variance) + 1))\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNumber of components needed for 95% variance: {np.argmax(cumulative_variance >= 0.95) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Transform the Data\n",
    "\n",
    "Now we project the original data onto the principal components. Let's reduce to 2 dimensions for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 2 principal components\n",
    "n_components = 2\n",
    "W = eigenvectors_sorted[:, :n_components]  # Matrix of first 2 eigenvectors\n",
    "\n",
    "print(\"Shape of W (projection matrix):\", W.shape)\n",
    "print(\"\\nFirst 2 Principal Components (eigenvectors):\")\n",
    "print(W)\n",
    "\n",
    "# Project the data onto the principal components\n",
    "X_pca_manual = X_standardized @ W\n",
    "\n",
    "print(\"\\nShape of transformed data:\", X_pca_manual.shape)\n",
    "print(\"\\nFirst few transformed samples:\")\n",
    "print(X_pca_manual[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the PCA Results\n",
    "\n",
    "Let's visualize the data in the new 2D principal component space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(X_pca_manual, columns=['PC1', 'PC2'])\n",
    "pca_df['species'] = pd.Categorical.from_codes(y, target_names)\n",
    "\n",
    "# Scatter plot of PCA results\n",
    "fig = px.scatter(pca_df, x='PC1', y='PC2', color='species',\n",
    "                 title='Iris Dataset Projected onto First 2 Principal Components (Manual PCA)',\n",
    "                 labels={'PC1': f'PC1 ({variance_explained[0]*100:.1f}% variance)',\n",
    "                        'PC2': f'PC2 ({variance_explained[1]*100:.1f}% variance)'},\n",
    "                 height=500)\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nTotal variance captured by 2 components: {cumulative_variance[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the Principal Components\n",
    "\n",
    "Let's examine the loadings (coefficients) of each original feature in the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame showing the contribution of each feature to each PC\n",
    "loadings_df = pd.DataFrame(\n",
    "    W.T,\n",
    "    columns=feature_names,\n",
    "    index=['PC1', 'PC2']\n",
    ")\n",
    "\n",
    "print(\"Feature Loadings (contribution of each feature to PCs):\")\n",
    "print(loadings_df)\n",
    "\n",
    "# Visualize loadings as a heatmap\n",
    "fig = px.imshow(loadings_df, \n",
    "                labels=dict(x=\"Feature\", y=\"Principal Component\", color=\"Loading\"),\n",
    "                title=\"Feature Loadings on Principal Components\",\n",
    "                color_continuous_scale='RdBu_r',\n",
    "                aspect='auto', height=300)\n",
    "fig.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"PC1: All features contribute positively, especially petal length and width.\")\n",
    "print(\"     This component captures the overall size of the flower.\")\n",
    "print(\"PC2: Sepal length and width contribute in opposite directions.\")\n",
    "print(\"     This component captures the shape variation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: PCA Using Scikit-learn\n",
    "\n",
    "Now let's implement the same analysis using scikit-learn's PCA implementation. This is more efficient and handles edge cases better.\n",
    "\n",
    "### Apply PCA with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Standardized data mean:\", X_scaled.mean(axis=0))\n",
    "print(\"Standardized data std:\", X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca_sklearn = PCA(n_components=2)\n",
    "X_pca_sklearn = pca_sklearn.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Shape of transformed data:\", X_pca_sklearn.shape)\n",
    "print(\"\\nVariance explained by each component:\")\n",
    "print(pca_sklearn.explained_variance_ratio_)\n",
    "print(\"\\nCumulative variance explained:\")\n",
    "print(np.cumsum(pca_sklearn.explained_variance_ratio_))\n",
    "print(\"\\nPrincipal Components (eigenvectors):\")\n",
    "print(pca_sklearn.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results\n",
    "\n",
    "Let's verify that our manual implementation matches scikit-learn's results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance explained (manual vs sklearn):\")\n",
    "print(\"Manual:\", variance_explained[:2])\n",
    "print(\"Sklearn:\", pca_sklearn.explained_variance_ratio_)\n",
    "print(\"\\nAre variance ratios close?\", np.allclose(variance_explained[:2], pca_sklearn.explained_variance_ratio_))\n",
    "\n",
    "# Note: The signs of eigenvectors might be flipped, but they represent the same direction\n",
    "print(\"\\nPrincipal components are the same (accounting for possible sign flips):\")\n",
    "for i in range(2):\n",
    "    manual_pc = W[:, i]\n",
    "    sklearn_pc = pca_sklearn.components_[i]\n",
    "    # Check if they are the same or opposite (both are valid)\n",
    "    same_or_opposite = np.allclose(manual_pc, sklearn_pc) or np.allclose(manual_pc, -sklearn_pc)\n",
    "    print(f\"PC{i+1}: {same_or_opposite}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Scikit-learn Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for sklearn PCA results\n",
    "pca_sklearn_df = pd.DataFrame(X_pca_sklearn, columns=['PC1', 'PC2'])\n",
    "pca_sklearn_df['species'] = pd.Categorical.from_codes(y, target_names)\n",
    "\n",
    "# Scatter plot\n",
    "fig = px.scatter(pca_sklearn_df, x='PC1', y='PC2', color='species',\n",
    "                 title='Iris Dataset Projected onto First 2 Principal Components (Scikit-learn PCA)',\n",
    "                 labels={'PC1': f'PC1 ({pca_sklearn.explained_variance_ratio_[0]*100:.1f}% variance)',\n",
    "                        'PC2': f'PC2 ({pca_sklearn.explained_variance_ratio_[1]*100:.1f}% variance)'},\n",
    "                 height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA for Different Numbers of Components\n",
    "\n",
    "Let's explore how many components we need to capture different amounts of variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with all components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Find number of components for different variance thresholds\n",
    "cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "print(\"Number of components needed for:\")\n",
    "for threshold in [0.80, 0.90, 0.95, 0.99]:\n",
    "    n_comp = np.argmax(cumvar >= threshold) + 1\n",
    "    actual_var = cumvar[n_comp - 1]\n",
    "    print(f\"  {threshold*100:.0f}% variance: {n_comp} components (actual: {actual_var*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Automatically Select Components\n",
    "\n",
    "Scikit-learn can automatically select the number of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to retain 95% of variance\n",
    "pca_auto = PCA(n_components=0.95)  # Retain 95% variance\n",
    "X_pca_auto = pca_auto.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Number of components selected: {pca_auto.n_components_}\")\n",
    "print(f\"Total variance explained: {np.sum(pca_auto.explained_variance_ratio_)*100:.2f}%\")\n",
    "print(f\"\\nVariance explained by each component:\")\n",
    "for i, var in enumerate(pca_auto.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Transform: Reconstructing Original Data\n",
    "\n",
    "One interesting property of PCA is that we can reconstruct an approximation of the original data from the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and then inverse transform\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "X_reconstructed = pca_2d.inverse_transform(X_pca_2d)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
    "print(f\"Reconstruction error (MSE): {reconstruction_error:.6f}\")\n",
    "\n",
    "# Compare original and reconstructed data for first sample\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Original (scaled)': X_scaled[0],\n",
    "    'Reconstructed': X_reconstructed[0],\n",
    "    'Difference': X_scaled[0] - X_reconstructed[0]\n",
    "})\n",
    "print(\"\\nComparison for first sample:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for 3D Visualization\n",
    "\n",
    "Let's create a 3D visualization using the first three principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with 3 components\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "pca_3d_df = pd.DataFrame(X_pca_3d, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_3d_df['species'] = pd.Categorical.from_codes(y, target_names)\n",
    "\n",
    "fig = px.scatter_3d(pca_3d_df, x='PC1', y='PC2', z='PC3', color='species',\n",
    "                    title='Iris Dataset in 3D PCA Space',\n",
    "                    labels={'PC1': f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "                           'PC2': f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.1f}%)',\n",
    "                           'PC3': f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.1f}%)'},\n",
    "                    height=600)\n",
    "fig.show()\n",
    "\n",
    "print(f\"Total variance explained by 3 components: {np.sum(pca_3d.explained_variance_ratio_)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Implemented PCA from scratch** using NumPy:\n",
    "   - Standardized the data\n",
    "   - Computed the covariance matrix\n",
    "   - Found eigenvalues and eigenvectors\n",
    "   - Selected principal components based on explained variance\n",
    "   - Transformed the data to the new coordinate system\n",
    "\n",
    "2. **Used scikit-learn's PCA implementation**:\n",
    "   - Verified our manual implementation\n",
    "   - Explored automatic component selection\n",
    "   - Demonstrated inverse transformation\n",
    "   - Created 2D and 3D visualizations\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- PCA reduces dimensionality while preserving as much variance as possible\n",
    "- The first 2 principal components capture ~95% of the variance in the Iris dataset\n",
    "- Principal components are orthogonal linear combinations of original features\n",
    "- PCA makes it easier to visualize and understand high-dimensional data\n",
    "- Always standardize data before applying PCA to ensure all features contribute equally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
