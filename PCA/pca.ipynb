{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51a22b0",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used in machine learning and statistics. It transforms high-dimensional data into a lower-dimensional form while retaining as much variance (information) as possible.\n",
    "\n",
    "PCA is particularly useful when:\n",
    "\n",
    "  + You have many features and want to reduce computational complexity\n",
    "  + You need to visualize high-dimensional data in 2D or 3D\n",
    "  + Features are correlated and you want to eliminate redundancy\n",
    "  + You want to reduce noise and improve model performance\n",
    "\n",
    "In this chapter, we will explore the mathematical foundations of PCA, connecting to concepts from linear algebra and calculus, and demonstrate how to apply PCA in practice.\n",
    "\n",
    "\n",
    "## Why Dimensionality Reduction?\n",
    "\n",
    "In many real-world datasets, we encounter the \"curse of dimensionality\" - as the number of features increases:\n",
    "\n",
    "  + **Computational cost** grows exponentially\n",
    "  + **Visualization** becomes impossible beyond 3 dimensions\n",
    "  + **Model complexity** increases, leading to overfitting\n",
    "  + **Feature correlation** may introduce redundancy\n",
    "\n",
    "PCA addresses these challenges by finding a new set of uncorrelated variables (principal components) that capture the maximum variance in the data.\n",
    "\n",
    "\n",
    "## Mathematical Foundations of PCA\n",
    "\n",
    "PCA relies on concepts from linear algebra and calculus. Understanding the mathematical foundation will help you appreciate how PCA works and when to use it.\n",
    "\n",
    "### Variance and Covariance\n",
    "\n",
    "**Variance** measures how much a single variable spreads out from its mean:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\frac{1}{n-1} \\sum_{i=1}^{n}(x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "**Covariance** measures how two variables change together:\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "  + $x_i$ and $y_i$ are individual data points\n",
    "  + $\\bar{x}$ and $\\bar{y}$ are the means of $X$ and $Y$\n",
    "  + $n$ is the number of observations\n",
    "\n",
    "The **covariance matrix** $\\Sigma$ for a dataset with $p$ features is a $p \\times p$ symmetric matrix where:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_p) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_p) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_p, X_1) & \\text{Cov}(X_p, X_2) & \\cdots & \\text{Var}(X_p)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "PCA finds the directions of maximum variance by computing the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "For a square matrix $A$, a vector $\\mathbf{v}$ is an **eigenvector** and $\\lambda$ is its corresponding **eigenvalue** if:\n",
    "\n",
    "$$\n",
    "A\\mathbf{v} = \\lambda\\mathbf{v}\n",
    "$$\n",
    "\n",
    "This means that when matrix $A$ is applied to eigenvector $\\mathbf{v}$, the vector only gets scaled by $\\lambda$ without changing direction.\n",
    "\n",
    "To find eigenvalues, we solve the **characteristic equation**:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "  + $\\det$ is the determinant\n",
    "  + $I$ is the identity matrix\n",
    "  + $\\lambda$ are the eigenvalues that satisfy this equation\n",
    "\n",
    "**In the context of PCA:**\n",
    "\n",
    "  + **Eigenvectors** of the covariance matrix represent the directions (principal components) of maximum variance\n",
    "  + **Eigenvalues** represent the amount of variance explained by each principal component\n",
    "  + Larger eigenvalues correspond to more important principal components\n",
    "\n",
    "\n",
    "## The PCA Algorithm\n",
    "\n",
    "PCA follows these steps to transform the data:\n",
    "\n",
    "### Step 1: Standardize the Data\n",
    "\n",
    "First, we center the data by subtracting the mean from each feature:\n",
    "\n",
    "$$\n",
    "X_{\\text{centered}} = X - \\mu\n",
    "$$\n",
    "\n",
    "Often, we also scale the data to unit variance (z-score normalization):\n",
    "\n",
    "$$\n",
    "X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where $\\mu$ is the mean vector and $\\sigma$ is the standard deviation vector.\n",
    "\n",
    "**Why standardize?** Features with larger scales would dominate the principal components. Standardization ensures each feature contributes equally.\n",
    "\n",
    "### Step 2: Compute the Covariance Matrix\n",
    "\n",
    "Calculate the covariance matrix of the standardized data:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X^T X\n",
    "$$\n",
    "\n",
    "Where $X$ is the centered/standardized data matrix ($n \\times p$), with $n$ samples and $p$ features.\n",
    "\n",
    "### Step 3: Compute Eigenvalues and Eigenvectors\n",
    "\n",
    "Solve the eigenvalue problem for the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\n",
    "$$\n",
    "\n",
    "This yields $p$ eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_p$ and corresponding eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_p$.\n",
    "\n",
    "### Step 4: Sort Eigenvalues and Select Principal Components\n",
    "\n",
    "Sort eigenvalues in descending order:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_p\n",
    "$$\n",
    "\n",
    "Select the top $k$ eigenvectors corresponding to the $k$ largest eigenvalues. These form the **principal components**.\n",
    "\n",
    "### Step 5: Transform the Data\n",
    "\n",
    "Project the original data onto the selected principal components:\n",
    "\n",
    "$$\n",
    "Z = X W_k\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "  + $Z$ is the transformed data ($n \\times k$)\n",
    "  + $X$ is the standardized original data ($n \\times p$)\n",
    "  + $W_k$ is the matrix of $k$ selected eigenvectors ($p \\times k$)\n",
    "\n",
    "\n",
    "## Variance Explained\n",
    "\n",
    "An important aspect of PCA is understanding how much information (variance) is retained after dimensionality reduction.\n",
    "\n",
    "The **proportion of variance explained** by the $i$-th principal component is:\n",
    "\n",
    "$$\n",
    "\\text{Variance Explained}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}\n",
    "$$\n",
    "\n",
    "The **cumulative variance explained** by the first $k$ components is:\n",
    "\n",
    "$$\n",
    "\\text{Cumulative Variance}_k = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}\n",
    "$$\n",
    "\n",
    "**Rule of thumb:** Select enough principal components to explain at least 80-95% of the total variance.\n",
    "\n",
    "\n",
    "## Geometric Interpretation\n",
    "\n",
    "Geometrically, PCA can be understood as:\n",
    "\n",
    "1. **Finding new axes:** The principal components represent new orthogonal (perpendicular) axes in the feature space\n",
    "2. **Rotating the data:** PCA rotates the data so that the maximum variance lies along the first axis (PC1), the second maximum variance along the second axis (PC2), and so on\n",
    "3. **Projection:** The transformed data are the projections of the original data points onto these new axes\n",
    "\n",
    "This rotation aligns the data with the directions of maximum variance, making it easier to identify patterns and reduce dimensionality.\n",
    "\n",
    "\n",
    "## Connection to Calculus: Optimization Perspective\n",
    "\n",
    "PCA can also be viewed as an optimization problem. We want to find the direction $\\mathbf{w}$ that maximizes the variance of the projected data:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}} \\mathbf{w}^T \\Sigma \\mathbf{w} \\quad \\text{subject to} \\quad \\|\\mathbf{w}\\| = 1\n",
    "$$\n",
    "\n",
    "Using **Lagrange multipliers** from calculus, we form the Lagrangian:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, \\lambda) = \\mathbf{w}^T \\Sigma \\mathbf{w} - \\lambda(\\mathbf{w}^T\\mathbf{w} - 1)\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $\\mathbf{w}$ and setting it to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = 2\\Sigma\\mathbf{w} - 2\\lambda\\mathbf{w} = 0\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\Sigma\\mathbf{w} = \\lambda\\mathbf{w}\n",
    "$$\n",
    "\n",
    "This is exactly the eigenvalue equation! The solution is the eigenvector corresponding to the largest eigenvalue. Subsequent principal components are found by maximizing variance in directions orthogonal to previous components.\n",
    "\n",
    "\n",
    "## PCA in Python\n",
    "\n",
    "In Python, we can implement PCA using:\n",
    "\n",
    "  + **Manual implementation:** Using NumPy to compute covariance matrix, eigenvalues, and eigenvectors\n",
    "  + **Scikit-learn:** Using the [`PCA` class](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for efficient implementation\n",
    "\n",
    "### Basic Usage with Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Access explained variance\n",
    "print(\"Variance explained by each component:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Access principal components (eigenvectors)\n",
    "print(\"\\nPrincipal components:\")\n",
    "print(pca.components_)\n",
    "```\n",
    "\n",
    "\n",
    "## When to Use PCA\n",
    "\n",
    "PCA is particularly useful when:\n",
    "\n",
    "  + **Reducing computational cost:** With hundreds or thousands of features\n",
    "  + **Visualization:** Reducing to 2-3 dimensions for plotting\n",
    "  + **Removing multicollinearity:** When features are highly correlated\n",
    "  + **Noise reduction:** As minor components often represent noise\n",
    "  + **Feature extraction:** Creating new features that capture the most important patterns\n",
    "\n",
    "**Important considerations:**\n",
    "\n",
    "  + PCA assumes **linear relationships** between features\n",
    "  + The transformed features (principal components) are **linear combinations** of original features, which may be harder to interpret\n",
    "  + PCA is sensitive to **scaling**, so always standardize your data first\n",
    "  + PCA is an **unsupervised** technique - it doesn't consider the target variable\n",
    "\n",
    "\n",
    "## Applications of PCA\n",
    "\n",
    "PCA is widely used across various domains:\n",
    "\n",
    "  + **Image Processing:** Facial recognition, image compression\n",
    "  + **Genomics:** Analyzing gene expression data\n",
    "  + **Finance:** Portfolio risk analysis, detecting patterns in stock prices\n",
    "  + **Natural Language Processing:** Topic modeling, document similarity\n",
    "  + **Computer Vision:** Object detection, image reconstruction\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "Principal Component Analysis is a powerful dimensionality reduction technique that:\n",
    "\n",
    "  + Transforms data into a new coordinate system where axes represent directions of maximum variance\n",
    "  + Uses eigenvectors and eigenvalues of the covariance matrix to identify these directions\n",
    "  + Can be understood through optimization using Lagrange multipliers from calculus\n",
    "  + Helps reduce computational complexity, visualize data, and eliminate feature redundancy\n",
    "  + Requires standardization and careful selection of the number of components to retain\n",
    "\n",
    "In the next section, we will explore practical implementations of PCA, both from scratch and using Python libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
